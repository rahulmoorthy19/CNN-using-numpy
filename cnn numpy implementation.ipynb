{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_width=32\n",
    "image_height=32\n",
    "image_channel=1\n",
    "batch_size=32\n",
    "learning_rate=0.009\n",
    "mu=0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_initializer():\n",
    "    W1=np.random.randn(3,3,4)*0.01  #multplying 0.01 so that the weights and biases are small at start\n",
    "    b1=np.random.randn(1,1,4)*0.01\n",
    "    \n",
    "    W2=np.random.randn(16,16,4)*0.01\n",
    "    b2=np.random.randn(1,1,4)*0.01\n",
    "\n",
    "    W3=np.random.randn(1024,10)*0.01\n",
    "    b3=np.random.randn(10,1)*0.01\n",
    "    \n",
    "    \n",
    "    params={'W1':W1,'b1':b1,'W2':W2,'b2':b2,'W3':W3,'b3':b3}\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_single_step(a_slice_prev,W,b):\n",
    "    \"\"\"\n",
    "    Apply one filter defined by parameters W on a single slice (a_slice_prev) of the output activation \n",
    "    of the previous layer.\n",
    "    \n",
    "    argument:\n",
    "    a_slice_prev=previous layer output\n",
    "    W=filter(window,window,channels)\n",
    "    b=bias\n",
    "    \"\"\"\n",
    "    s=a_slice_prev*W\n",
    "    Z=np.sum(s)\n",
    "    Z=Z+float(b)\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_forward(a_prev,w,b,stride):\n",
    "    \"\"\"\n",
    "    This is used for forward propagation of a convolution neural network\n",
    "    \n",
    "    arguments:\n",
    "    a_prev=previous layer output shape=(m,nh_prev,nw_prev,nc_prev)\n",
    "    w=filter shape=(nh,cw,nc_prev,nc)\n",
    "    bias= bias shape=(1,1,1,nc)\n",
    "    \"\"\"\n",
    "    (nh_prev,nw_prev,nc_prev)=a_prev.shape\n",
    "    (f,f,nc)=w.shape\n",
    "    nh=int((nh_prev-f)/stride)+1\n",
    "    nw=int((nw_prev-f)/stride)+1\n",
    "    Z=np.zeros((nh,nw,nc))\n",
    "    for h in range(nh):\n",
    "        for w in range(nw):\n",
    "            for c in range(nc):\n",
    "                vert_start = h*stride\n",
    "                vert_end = vert_start+f\n",
    "                horiz_start = w*stride\n",
    "                horiz_end = horiz_start+f\n",
    "                a_slice_prev = a_prev[vert_start:vert_end,horiz_start:horiz_end,:]\n",
    "                Z[h,w,c]=conv_single_step(a_slice_prev,W[...,c],b[...,c])\n",
    "    Z[Z<=0]=0 #relu layer\n",
    "    cache=(a_prev,w,b)\n",
    "    return Z,cache\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_cost(out,label):            \n",
    "    \"\"\"\n",
    "    softmax layer \n",
    "    \"\"\"\n",
    "    eout=np.exp(out,dtype=np.float)\n",
    "    probs=eout/sum(eout)\n",
    "    p=sum(label*probs)\n",
    "    cost=-np.log(p)\n",
    "    return cost,probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_backward(dz,cache,stride):\n",
    "    \"\"\"\n",
    "    backpropagation for convolution layer\n",
    "    \n",
    "    arguments:\n",
    "    dz=desent of previous layer\n",
    "    cache=values of bias,weight of the layer during the forward pass\n",
    "    \n",
    "    returns:\n",
    "    da_prev=desent used for calculating the dw,db of the next layer\n",
    "    dw=correction of the weights\n",
    "    db=correction of the bias\n",
    "    \"\"\"\n",
    "    (A_prev,w,b,hparameters)=cache\n",
    "    (nh_prev,nw_prev,nc_prev)=a_prev.shape\n",
    "    (f,f,nc)=W.shape\n",
    "    (nh,nw,nc)=dz.shape\n",
    "    da_prev=np.zeros((nh_prev,nw_prev,nc_prev))\n",
    "    dw=np.zeros((f,f,nc))\n",
    "    db=np.zeros((1,1,nc))\n",
    "    for h in range(nh):\n",
    "        for w in range(nw):\n",
    "            for c in range(nc):\n",
    "                vert_start = h*stride\n",
    "                vert_end = vert_start+f\n",
    "                horiz_start = w*stride\n",
    "                horiz_end = horiz_start+f\n",
    "                da_prev[vert_start:vert_end, horiz_start:horiz_end, :]+= W[:,:,c] * dz[h, w, c]\n",
    "                dw[:,:,c]=a_prev[vert_start:vert_end,horiz_start:horiz_end,:]*dz[h,w,c]\n",
    "                db[:,:,c]+=dz[h,w,c]\n",
    "    da_prev[da_prev<=0]=0#relu \n",
    "    return da_prev,dw,db\n",
    "                    \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolution(image,label,params):\n",
    "    \"\"\"\n",
    "    As all the helper function are created so this the complilation both front and backprop:\n",
    "    \n",
    "    input->convlayer1(3,3,4)->convlayer2(16,16,4)->fullyconnected(1024,10)->fullyconnected(10,1)->output\n",
    "    \"\"\"\n",
    "    (image_height,image_width,image_channel)=image.shape\n",
    "    conv1,conv1_cache=conv_forward(image,params[\"W1\"],params[\"b1\"],2)\n",
    "    conv2,conv2_cache=conv_forward(conv1,params[\"W2\"],params[\"b2\"],2)\n",
    "    fc1=conv2.reshape((1024,1)) #1024*1\n",
    "    fc2=params[\"W3\"].dot(fc1)+params[\"b3\"] #10*1\n",
    "        \n",
    "        #backprop\n",
    "    cost,probs=softmax_cost(fc2,label)\n",
    "    dout=probs-label\n",
    "    dw3=dout.dot(fc1.T)\n",
    "    db3=(dount.T).T.reshape((1024,1))\n",
    "    dfc1=params[\"W3\"].T.dot(dout)\n",
    "    da2_prev,dw2,db2=conv_backward(dfc1.T.reshape(conv2.shape),conv2_cache,2)\n",
    "    da1_prev,dw1,db1=conv_backward(da2_prev,conv1_cache,2)\n",
    "    return dw1,db1,dw2,db2,dw3,db3,cost\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update():\n",
    "    \"\"\"\n",
    "    used update as a different function to show the update of batch gradient descent else we can directly update in \n",
    "    backpropagation function\n",
    "    \n",
    "    mu is a hyperparameter which helps in regularization\n",
    "    \"\"\"\n",
    "    finalcost=[]\n",
    "    params=parameter_initializer() \n",
    "    for i in range(0,batch_size):\n",
    "        dw1,db1,dw2,db2,dw3,db3,cost=backpropagation(image,label,params)\n",
    "        params[\"W1\"]+=mu*params[\"W1\"]-(learning_rate*dw1/batch_size)\n",
    "        params[\"b1\"]+=mu*params[\"b1\"]-(learning_rate*db1/batch_size)\n",
    "        params[\"W2\"]+=mu*params[\"W2\"]-(learning_rate*dw2/batch_size)\n",
    "        params[\"b2\"]+=mu*params[\"b2\"]-(learning_rate*db2/batch_size)\n",
    "        params[\"W3\"]+=mu*params[\"W3\"]-(learning_rate*dw3/batch_size)\n",
    "        params[\"b3\"]+=mu*params[\"b3\"]-(learning_rate*db3/batch_size)\n",
    "        finalcost.append(cost/batch_size)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
